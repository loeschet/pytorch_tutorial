{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsV76W0PjfCs"
      },
      "source": [
        "# Machine learning tutorial - Exercise\n",
        "\n",
        "Machine learning (ML) has seen rapid growth in the past decade and has revolutionized many fields in both industry and research, achieving state-of-the-art results and solving highly complex problems. Due to the sheer vastness of this topic, it is impossible to give a full overview in a single tutorial.\n",
        "\n",
        "Thus, we are focussing on equipping you with all the basic tools necessary to start into the field of ML and be able to utilize its power for your own research project. In particular, we will use one of the most popular ML frameworks, [pytorch](https://pytorch.org/), to solve two typical tasks: classification and regression.\n",
        "\n",
        "## Choosing a dataset\n",
        "\n",
        "There exist several curated \"benchmark\" datasets that have been used in many papers and competitions for comparing the performance of different machine learning algorithms. Since we assume the audience to be interested in scientific datasets in particular, we chose the [Higgs dataset](https://archive.ics.uci.edu/ml/datasets/HIGGS) for our classification task. For the regression task, we chose the [life expectancy dataset](https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who) by the World Health Organisation (WHO). Let's get started!\n",
        "\n",
        "**Tip: There are several interesting websites to look for openly available benchmark datasets, like [google dataset search](https://datasetsearch.research.google.com/), [kaggle](https://www.kaggle.com/datasets/) or [paperswithcode](https://paperswithcode.com/datasets). There are also built-in datasets that you can easily access using the pytorch API, see [this Link](https://pytorch.org/vision/stable/datasets.html) for more information or [this Link](https://www.tensorflow.org/datasets/catalog/overview) for the `TensorFlow` equivalent.** \n",
        "\n",
        "# Part 1: Regression task\n",
        "\n",
        "To learn about the fundamentals of `pytorch`, we will start with a multivariate regression task. We use the WHO life [life expectancy dataset](https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who) where we try to predict the life expectancy of a country given different medical, economical and social factors.\n",
        "\n",
        "\n",
        "## Downloading the WHO life expectancy dataset\n",
        "\n",
        "The dataset can be downloaded from the above kaggle link, or simply by typing the following command in a command shell:\n",
        "\n",
        "```\n",
        "wget -O life_expectancy_data.csv https://wolke.physnet.uni-hamburg.de/index.php/s/Bdnmoit483Xrnmo/download\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing the WHO dataset\n",
        "\n",
        "Before we actually start with the regression, we do some initial exploratory data analysis first. This is important to get an idea of what the data looks like and check if we have outliers or missing values.\n",
        "\n",
        "A useful library for data analysis is [pandas](https://pandas.pydata.org/), which stores data in a so called \"data frame\". A data frame is essentially a table, where each row represents a sample of our dataset and each column represents a specific feature of this sample. For example, in case of the WHO dataset, each row represents a country and its respective data for a particular year and each column represents a specific economic or demographic feature, such as population or GDP.\n",
        "\n",
        "The pandas library also comes with built-in plotting functions and allows for efficient exploration, preprocessing and manipulation of our data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load data from csv file\n",
        "df_who = pd.read_csv(\"life_expectancy_data.csv\")\n",
        "\n",
        "# take a look at dataframe\n",
        "df_who"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, this dataset consists of 2938 rows and 22 columns. We also see some of the column names that hint at what information they contain. The first column, `Country` contains the country name. We see that some countries occur multiple times. This is because the dataset contains data from the years 2000 to 2015 and the data of each year is put in a separate row for each country. The `Status` column tells us about the development status of a country. Then, we see several columns with health-related information, such as the `Life excpectancy`, the `Adult Mortality`, Alcohol consumption etc.\n",
        "\n",
        "\n",
        "Let us explore this dataset a bit before we start with the analysis. We can make use of the plotting functions contained within `pandas` for this. Looking at all the 22 features of this dataset will take quite some time, so for this tutorial, we focus only on a subset of them. However, in your own research we recommend checking all the variables you want to analyze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# columns of pandas dataframes can be accessed similar to dictionary\n",
        "# values in python, by stating my_df[\"column_name\"]. If you access a\n",
        "# single column this way, you get a pandas Series object. If you pass\n",
        "# multiple column names as a list (i.e.\n",
        "# my_df[[\"column_name_1\", \"column_name_2\"]]) it wil return a new DataFrame\n",
        "# object with the selected columns only.\n",
        "\n",
        "# we will select four variables for this test\n",
        "selected_columns = [\"Life expectancy\", \"Adult Mortality\", \"Alcohol\", \"GDP\"]\n",
        "\n",
        "# get new DataFrame object with the selected columns only\n",
        "df_subset = df_who[selected_columns]\n",
        "\n",
        "df_subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's plot the distributions of these variables. In particular, let's take a look at developed and developing countries separately. We can achieve this using the `Status` column in the original DataFrame object, `df_who`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_who[\"Status\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `value_counts` method is very useful if we want to know about the unique values in a column and how often they occur. We see that a country can have either the value `Developed` or `Developing`.\n",
        "\n",
        "We can simply create a selection mask from that column and use `pandas` built-in plotting methods to compare the distributions of our selected column variables for developed and developing countries separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create mask for developed/developing country comparison\n",
        "dev_mask = (df_who['Status'] == \"Developed\")\n",
        "\n",
        "# Rearrange selected columns to a 2x2 matrix (as numpy array)\n",
        "# to use in the subplots call (this arranges the plots of the\n",
        "# four selected features in a 2x2 way)\n",
        "colname_matrix = df_subset.columns.to_numpy().reshape(2,2)\n",
        "fig, axs = plt.subplots(2,2, figsize=(14, 7))\n",
        "\n",
        "for row in range(2):\n",
        "    for col in range(2):\n",
        "        colname_tmp = colname_matrix[row, col]\n",
        "\n",
        "        # set current axis to subplot index\n",
        "        plt.sca(axs[row, col])\n",
        "        \n",
        "        # define bin edges and use histogram plotting function\n",
        "        # of pandas dataframe\n",
        "        bin_edges = np.linspace(df_subset[colname_tmp].min(),\n",
        "                                df_subset[colname_tmp].max(), 50)\n",
        "        df_subset[~dev_mask][colname_tmp].plot.hist(bins=bin_edges,\n",
        "                                                    label=\"Developing\",\n",
        "                                                    density=True)\n",
        "\n",
        "        df_subset[dev_mask][colname_tmp].plot.hist(histtype=\"step\",\n",
        "                                                   bins=bin_edges,\n",
        "                                                   label=\"Developed\",\n",
        "                                                   density=True)\n",
        "\n",
        "        plt.xlabel(colname_tmp)\n",
        "        plt.legend(loc=\"upper right\")\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, developed countries have a significantly higher life expectancy and lower adult mortality than most developing countries. Interestingly, the alcohol consumption seems higher in Developed countries as well. The GDP distribution of Developed countries leans more towards higher values in the \"tail\" while devloping countries usually have a low GDP.\n",
        "\n",
        "All of these values are more or less what one would naively expect, so we do not see any variable being distributed in a completely counter-intuitive way.\n",
        "\n",
        "Another important step before starting any analysis is to **look for outliers** in the data. Outliers can sometimes worsen your analysis results, or may point you to faulty datapoints. For example, let's say there is a faulty entry in the WHO dataset whose population value is very high (say, above the entire population on earth). Or, imagine you want to analyze only developing countries and the `Status` label was wrongly assigned such that you have a single developed country in your dataset. In this case, outlier analysis helps you to pinpoint these entries (in the former case by looking at the `Population` column, in the latter case by finding a very high value e.g. in the `GDP` column) and remove them from the dataset.\n",
        "\n",
        "To visually assess if there are outliers in our WHO dataset, we can use the box plot method of our `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# as before: arrange plots in 2x2 matrix\n",
        "colname_matrix = df_subset.columns.to_numpy().reshape(2,2)\n",
        "fig, axs = plt.subplots(2,2, figsize=(10, 7))\n",
        "\n",
        "for row in range(2):\n",
        "    for col in range(2):\n",
        "        colname_tmp = colname_matrix[row, col]\n",
        "        \n",
        "        # set current axis to subplot index\n",
        "        plt.sca(axs[row, col])\n",
        "        \n",
        "        df_subset[colname_tmp].plot.box()\n",
        "        \n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Box plots are a great way of investigating important features of a distribution at a glance: The outer edges of the box correspond to the first and third quartiles (i.e. the 25th and 75th percentiles). The green line represents the median and the \"whiskers\" are set to 1.5 $\\cdot$ IQR (IQR=\"Inter quartile range=Q3-Q1). If the highest or lowest datapoint of the distribution lie below the value of 1.5 $\\cdot$ IQR, the whiskers are simply put at that value. The points beyond the whiskers are the ones that exceed 1.5 $\\cdot$ IQR and thus could be considered as outliers.\n",
        "\n",
        "We see quite a few outliers in the GDP and Adult Mortality features, which is expected in distributions that have long tails, which is actually the case if we take into account the previous distribution plots. Still, the few very high points in the `GDP` column look a bit suspicious, so let's take a look just to be sure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_who[[\"Country\", \"GDP\"]].sort_values(by=\"GDP\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the highest values come from Luxembourg, which is known to rank amongst the highest GPD (per capita) countries in the world, so this seems to be a correct value and not an actual outlier.\n",
        "\n",
        "## Preprocessing the WHO dataset\n",
        "\n",
        "### Step 1: Encode non-numerical features\n",
        "\n",
        "We now already had a look at some of the distributions and checked for outliers. However, in order for the data to be used in a machine learning model, we need to clean it and put it in a representation that such a model can understand. Machine learning models can be trained on **numerical** inputs, which can be integers or floats. However, we see that there are at least some columns (e.g. the `Status` column) that have non-numerical values. \n",
        "\n",
        "We can easily check all the datatypes of our columns using the `dtypes` variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_who.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that we are lucky: Most columns have a numeric type, like `int` or `float`. `String` columns typically have the `object` type in pandas dataframes. We see that there are two string-encoded columns: the `Country` column, which simply contains the name of the country and the `Status` column, which tells us about the development status of a country.\n",
        "\n",
        "For a machine learning model to work on this data, we would need to turn these two columns into a numerical representation. This means we need to employ some kind of **encoding**. This is a topic for itself, but in principle there exist two major ways how we can encode strings that are categorical (i.e. there exist a limited number of unique string values, such as the country names):\n",
        "\n",
        "1. Ordinal encoding: If there is an intrinsic order to the strings, we can simply convert the string values into integers, ranging from 1 to the number of unique string values. For example, say our column was \"shirt size\" and we had the values \"S\", \"M\", \"L\", \"XL\" and \"XXL\", then we could just encode them with integers from 1 to 5. This is possible because the size strings carry meaning: \"S\" is a smaller size than \"XL\", which is why it makes sense to assign it a smaller value.\n",
        "\n",
        "2. One-hot encoding: If there is no intrinsic order to the strings (for example in case of our country names), we can do what is called a [one-hot encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/). In a one-hot encoding, we **create a new column for each unique string value in the original column** and this column is 1 where this unique value occurs and 0 everywhere else. For example, let's imagine our country names would only contain the names of three countries: France, Germany and Belgium. To one-hot encode this column, we would create three new columns: one column is 1 where the string \"France\" occurs in the original column and else it is 0. The second column is 1 where the string \"Germany\" occurs and else it is again 0. And the same for the \"Belgium\" string. This can of course inflate our dataset quite a bit, in particular if we have many unique values.\n",
        "\n",
        "Let's investigate the two `object` columns a bit. We can use the `value_counts` method to get the unique values in a column and how often they occur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_who[\"Country\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that there are 193 countries in that column. Some occur 16 times, which means they contain individual rows for each of the years 2000 to 2015, others only occur once (probably due to missing data in several years). If we were to one-hot encode this column, we would add 193 values, which seems a bit much. For now we decide to postpone the decision what to do with this column and just drop it for the first test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop Country column for now\n",
        "df_who = df_who.drop(columns=[\"Country\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's take a look at the `Status` column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_who[\"Status\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Status` column only has two unique values: `Developed` or `Developing`. For this column, one could argue to use an ordinal encoding, since developed and developing countries have certain unique quantifiable differences in terms of economics and also in other areas. This could lead us to use a higher integer value for developed countries and a lower one for still developing ones. However, since there are only two unique values in this column, we go for a one-hot encoding here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# simple pandas function to one-hot encode a column: get_dummies\n",
        "\n",
        "encoded_status = pd.get_dummies(df_who[\"Status\"], prefix=\"status\")\n",
        "\n",
        "encoded_status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that there are now two new columns: one for the developed and one for developing. We could actually just drop one of these columns, since they are perfectly correlated, so the whole information is already contained only in one of them. Note that this would result in a single column that is 0 for one value and 1 for the other one, which could also be interpreted as an ordinal encoding of the original feature. So if there are only two features, a one-hot encoding already has kind of an ordinal structure to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop one of the new columns\n",
        "encoded_status = encoded_status.drop(columns=[\"status_Developing\"])\n",
        "\n",
        "# drop the original status columns from the WHO DataFrame\n",
        "df_who = df_who.drop(columns=[\"Status\"])\n",
        "\n",
        "# add the new status_Developed column to the WHO DataFrame\n",
        "df_who[\"status_Developed\"] = encoded_status[\"status_Developed\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's again check the datatypes in our `DataFrame`. We should see that it doesn't contain `object` dtypes any more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_who.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cleaning the data\n",
        "\n",
        "Another important point in any data analysis task is to check for infinite (`INF`) and not-a-number (`NaN`) values. These values can for example occur if data is missing or if a faulty measurement was obtained. We can use our data frame in combination with the `numpy` function `isfinite`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# There are is a function called isna() that we can call on pandas dataframes\n",
        "# to check for NaN values. For INFs we could use isin([-np.inf, np.inf]), but\n",
        "# it is easier to use the numpy function isfinite() to check for both at the\n",
        "# same time.\n",
        "\n",
        "# We sum over the boolean array which is 1 at all occurrences that are\n",
        "# Nan or INF per column to count them\n",
        "(~np.isfinite(df_who)).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that we have several columns where there are multiple `NaN` or `INF` values. Lets check particularly for `INF` values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.isinf(df_who).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that there are no `INF` values, which means all the non-finite values we saw in the previous test are `NaN`.\n",
        "\n",
        "There are several ways how we can mitigate `NaN` values: If there are only few rows affected, we can simply drop these. However, this is usually not ideal since we also remove information from the data. If there exist columns where a majority of rows is `NaN`, we can also consider dropping the column, since it doesn't carry much information that we can use.\n",
        "\n",
        "Often, a better choice than simply dropping the values is **imputing** the values, i.e. replace the missing values with some other value. Typical choices for the replacement value are the mean or the median of the feature distribution. However, also these methods are not really great, since they do not preserve the relationships among variables. The optimal way would be to use some kind of domain knowledge to impute the values. For example, if you were an expert in the population growth behavior of developing countries and you knew that their population growth is best described with an exponential function, we could simply take the existing `Population` values of a country, do an exponential fit and set the missing values accordingly.\n",
        "\n",
        "Beyond that, there exist many other methods to impute missing values. You can read more on the topic for example [here](https://odsc.medium.com/data-imputation-beyond-mean-median-and-mode-6c798f3212e3). Another approach is based on the k-nearest neighbors algorithm: Here we impute the mean only of the $k$ neighboring data points, computed as the euclidean distance across all features. You can read more about this topic [here](https://machinelearningmastery.com/knn-imputation-for-missing-values-in-machine-learning/). We will use this algorithm here, since we can easily use the scikit-learn (`sklearn`) imputer called `KNNImputer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# use imputer defaults for now\n",
        "imputer = KNNImputer()\n",
        "\n",
        "# transform the dataframe and save it into a new DataFrame object\n",
        "df_who_clean = pd.DataFrame(imputer.fit_transform(df_who),\n",
        "                            columns=df_who.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check for `NaN` values again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(~np.isfinite(df_who_clean)).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It worked! We imputed all the `NaN` values using the kNN imputation approach! Now we are done with the first data cleaning step.\n",
        "\n",
        "\n",
        "### Preprocess samples for training\n",
        "\n",
        "Now comes another important step: We need to split our dataset into three parts: The **training** set, the **validation** set and the **test** set. These datasets need to be **completely separate** from each other, in order to guarantee statistically independent results. Let's quickly discuss why this is necessary:\n",
        "\n",
        "The training set is used for training the DNN. A DNN has many parameters or weights, that are adjusted during the training to best minimize a certain loss function - in case of the regression, this will be the **mean squared error** and in case of the binary classification, this will be the [**binary crossentropy**](https://en.wikipedia.org/wiki/Cross_entropy) loss. So in the end, we just fit a very complex function to our input dataset, which comes with a caveat: At first, the DNN will learn useful patterns in the data, but at some point the network will learn the \"noise\", i.e. the particular statistical fluctuations in our input distributions, to further decrease the loss. So the DNN just \"memorizes\" the data instead of learning the general pattern that helps solve our statistical problem.\n",
        "\n",
        "What we are interested in, however, is learning a general function that does not only perform well on the training set, but also *generalizes* well to other, previously unseen datasets or samples. This is why we need a second, statistically independend dataset when we evaluate our model. This is the **validation** set. With the validation set, we can give an estimate on the **generalization performance** of our trained model. The validation set is also useful to detect **overfitting**, which is another word describing the above mentioned phenomenon of the DNN model starting to learn the noise of the training set.\n",
        "\n",
        "In practice, we regularly evaluate our model both on the training and on the validation set during training. At first, the loss is minimized for both sets, but at some point the validation loss stalls out at a certain value or even increases, while the training loss goes further down. At this point, we can stop the training and select the model with the minimum validation loss, since it will yield the optimal generalization performance. This procedure is also referred to as **model selection**.\n",
        "\n",
        "However, also the model selection introduces a bias, since we select the model with the best generalization performance *on that particular validation set*. This is why we need a third statistically independent dataset, the **test set** to finally have an objective performance measure that is affected by neither the training nor the model selection procedure.\n",
        "\n",
        "Here is a summary figure of what we just discussed:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://s3.desy.de/hackmd/uploads/upload_4daa0520c9ce87a37f88ee55c5f2f82d.png\" width=60%>\n",
        "</p>\n",
        "\n",
        "There is no general rule how to optimally split up your dataset, but typically the majority of data will go into the training set, while test and validation set are kept roughly the same. We will choose a 70/15/15 percent split for our first test.\n",
        "\n",
        "Before we do that, we also split off the life expectany column from the `DataFrame`, since this is the \"target\" variable that we want to predict from all the other features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the cleaned DataFrame into targets and features\n",
        "targets = df_who_clean['Life expectancy']\n",
        "data = df_who_clean.drop(columns=['Life expectancy'])\n",
        "\n",
        "# Split into training/validation/test sets according to 70/15/15 split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split off training set (70% of samples)\n",
        "x_train, x_remain, y_train, y_remain = train_test_split(data, targets,\n",
        "                                                        train_size=0.7,\n",
        "                                                        random_state=42)\n",
        "\n",
        "\n",
        "# split remainder into test and validation sets\n",
        "# (15% each, corresponding to half of the remaining non-training samples)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_remain, y_remain,\n",
        "                                                train_size=0.5,\n",
        "                                                random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's see if the numbers check out\n",
        "n_full = data.shape[0]\n",
        "train_percent = (x_train.shape[0]/n_full)*100\n",
        "val_percent = (x_val.shape[0]/n_full)*100\n",
        "test_percent = (x_test.shape[0]/n_full)*100\n",
        "\n",
        "print(f\"Train set corresponds to {train_percent:.2f}% of the full data.\")\n",
        "print(f\"Validation set corresponds to {val_percent:.2f}% of the full data.\")\n",
        "print(f\"Test set corresponds to {test_percent:.2f}% of the full data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems everything checks out! Now we also would like to apply some more preprocessing to our data. Depending on your project, preprocessing - or \"data wrangling\" - can take up a significant proportion of your work. In the WHO dataset example we only apply a \"standard scaling\", which means that we subtract the mean and divide by the standard deviation in each feature, such that all features end up with a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "This makes sure our input features are all in a similar range, which can help algorithms based on gradient descent - which our DNN optimization is - have a faster and better convergence behaviour; (For more information read e.g. [this blog post](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/) for an in-depth discussion of feature scaling an when to use which scaling method)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import standard scaler from scikit-learn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit (i.e. do the mean/std computation based on the training set features)\n",
        "# and transform training data\n",
        "x_train = scaler.fit_transform(x_train).astype(\"float32\")\n",
        "\n",
        "# important: the standard scaling is always with respect to the training set\n",
        "# values of mean and std! So DO NOT call fit_trainsform again on the other sets\n",
        "# but just transform using the already fitted scaler.\n",
        "x_val = scaler.transform(x_val).astype(\"float32\")\n",
        "x_test = scaler.transform(x_test).astype(\"float32\")\n",
        "\n",
        "# Note: We also cast all values to 32 bit floats here, since this is what\n",
        "# pytorch models expect as inputs by default, while numpy arrays are stored\n",
        "# in 64 bit precision\n",
        "y_train = y_train.astype(\"float32\")\n",
        "y_val = y_val.astype(\"float32\")\n",
        "y_test = y_test.astype(\"float32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if it worked\n",
        "print(\"Means of training set:\\n\", x_train.mean(axis=0), \"\\n\")\n",
        "print(\"Standard deviations of training set:\\n\", x_train.std(axis=0), \"\\n\\n\")\n",
        "\n",
        "print(\"Means of validation set:\\n\", x_val.mean(axis=0), \"\\n\")\n",
        "print(\"Standard deviations of validation set:\\n\", x_val.std(axis=0), \"\\n\\n\")\n",
        "\n",
        "print(\"Means of test set:\\n\", x_test.mean(axis=0), \"\\n\")\n",
        "print(\"Standard deviations of test set:\\n\", x_test.std(axis=0), \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems it worked! While means and standard deviations are 0 and 1 respectively for the training set, we get slightly higher/lower values in the validation and test sets, which reflects the fact that we used the standard scaler based on the training set values only and then just applied the transformations to the other two sets.\n",
        "\n",
        "We are now good to go! We loaded our dataset, did some exploratory analysis, checked for outliers and `INF`/`NaN` values and then preprocessed it. Now we will build our neural network using `pytorch`!\n",
        "\n",
        "## Building the `pytorch` model\n",
        "\n",
        "Writing custom models in `pytorch` is relatively straightforward. For this tutorial, we will set up what is often referred to as a \"dense\" or \"fully connected\" neural network, which is essentially a model that consists of several layers. Each layer contains a certain number of nodes and all nodes from one layer are connected to all the nodes in the adjacent layers.\n",
        "\n",
        "These fully connected networks (FCNs) have an input layer and an output layer and so called \"hidden layers\" that are in between. The number of nodes in the input layer corresponds to the number of input features of our dataset. The output layer usually has one node for a regression problem, where we try to predict a single variable from different input variables. Also for binary classification problems there is a single node in the output layer, in which case the node outputs the probability of an event belonging to the \"positive\" class. However, there are also multi-class and multi-label classification problems, where multiple output nodes are necessary.\n",
        "\n",
        "Below you can find a simple picture of such a FCN architecture:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://s3.desy.de/hackmd/uploads/upload_7f05223770eb84859360fee088b1e88f.png\" width=60%>\n",
        "</p>\n",
        "\n",
        "The number of hidden layers and of nodes inside them is set by the user, since there is no \"optimal\" rule how many one should take. For one ML problem, fewer layers with many nodes are beneficial, while for other problems it might be the opposite. Varying and finding the setting of number of layers and nodes per layer that yields the best result for a given problem is often referred to as [\"hyperparameter optimization\"](https://en.wikipedia.org/wiki/Hyperparameter_optimization) and it is a task that needs to be re-done for each problem or network architecture. One can use several methods to tune hyperparameters, ranging from \"brute force\" methods such as [grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) or random search to more sophisticated algorithms based on Bayesian optimization. \n",
        "\n",
        "Common layers of neural networks are provided in the [`torch.nn`](https://pytorch.org/docs/stable/nn.html) module of `pytorch`. A fully connected layer can be implemented by using [`torch.nn.linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) and providing the number of input and output values of this layer. This means that, instead of providing the concrete number of nodes in the current layer, you also need to think about how many nodes the next layer should have.\n",
        "\n",
        "Let's take an example: Assume we had 5 input features and would like to create a layer for that. Our next layer, i.e. the first hidden layer, should have 16 nodes, which is why we would need to create the layer like this: `layer = nn.Linear(5, 16)`. Note that now the next linear layer in your model needs to have the same number of inputs as the output of the previous layer, so `16` in this case. You can now stack many of these layers like this until the output layer, which should have a number of output values of `1` for our regression problem.\n",
        "\n",
        "However, this is not the full story yet! In order for the FCN to be able to learn complex non-linear functions of the data, we need to apply an [activation function](https://en.wikipedia.org/wiki/Activation_function) to the outputs of our layers. The most common and useful activations are also contained in the `torch.nn` module and we can simply add them to our model. One common activation function is the [\"Rectified Linear Unit\"](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) (ReLU), which is zero at negative values values of $x$ and $x$ for values $x\\ge0$. We can add this activation by calling `torch.nn.ReLU()` after a linear layer. \n",
        "\n",
        "`ReLU` activations are especially useful in input and hidden layers. For the output layer, however, we need to choose a different activation function: For a regression problem, we would like to predict a continuous value, which means that we can simply use a linear activation in the end. Since we defined our output layer as a linear layer with one node already, we do not need to add a particular activation in this example.\n",
        "\n",
        "To put it all together, we can create a fully connected model in `pytorch` by stacking linear layers, followed by an activation (such as `ReLU`) and making sure that the activation of our output layer fits our problem at hand (e.g. linear activation for regression). In practice, we implement this appending layers and activations to a python `list` and once we are done, we create a model by providing the list to a [`torch.nn.Squential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) object, which is just a sequential container that builds our final model from our provided modules.\n",
        "\n",
        "In the code below, we have conveniently wrapped this into a separate class called `Regressor` which contains other useful methods that come in handy later in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Regressor(nn.Module):\n",
        "    def __init__(self, layers, n_inputs=5):\n",
        "        super().__init__()\n",
        "\n",
        "        # the layers variable will be the list that contains the individual\n",
        "        # linear layers and their activations\n",
        "        self.layers = []\n",
        "        for nodes in layers:\n",
        "            # in the first iteration, n_inputs will correspond to the number\n",
        "            # of input features in our dataset. After the input layer, the\n",
        "            # value of n_inputs is set to the number of output features, such\n",
        "            # that the layers are always stacked in the correct way\n",
        "            self.layers.append(nn.Linear(n_inputs, nodes))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            n_inputs = nodes\n",
        "\n",
        "        # the final output linear layer. We do not need an activation here,\n",
        "        # since for regression, we are interested in having a continuous\n",
        "        # output, which the linear layer already provides by default\n",
        "        self.layers.append(nn.Linear(n_inputs, 1))\n",
        "\n",
        "        # build pytorch model as sequence of our layers\n",
        "        self.model_stack = nn.Sequential(*self.layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # the forward call just takes data (x) and sends it through the model\n",
        "        # to produce an output\n",
        "        return self.model_stack(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        # the predict method sets the model to evaluation mode and only then\n",
        "        # computes the model prediction of given data (x). For convenience,\n",
        "        # we already make sure that the output prediction is a numpy array, so\n",
        "        # we can use it easier in the final evaluation of the model.\n",
        "        with torch.no_grad():\n",
        "            self.eval()\n",
        "            x = torch.tensor(x)\n",
        "            prediction = self.forward(x).detach().cpu().numpy()\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the training\n",
        "\n",
        "For running the training, we need to implement a \"training loop\". In Machine Learning, we typically optimize our model in an iterative way. This means that we use our training data multiple times to adjust the weights of the FCN and get better performance. Additionally, we often face the problem that our dataset is too large to fit into the memory at once. This is why usually we need to run the training in a \"batched\" fashion.\n",
        "\n",
        "For this, we use a tool called a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) that is included in `pytorch`. The `DataLoader` takes our full dataset and randomly samples small batches of data from it. We then loop over all the batches and for each batch result adjust the weights of the network until we looped over all the samples in our dataset this way. This is the \"inner training loop\" and once it concludes, we say that one \"epoch\" of training is finished.\n",
        "\n",
        "Also in the inner training loop, we need to use an [`optimizer`](https://pytorch.org/docs/stable/optim.html), which takes care of adjusting the model parameters to minimize the loss at each training step. There exist many optimization algorithms that one can use to approach the minimum of the loss function. Two very famous algorithms are [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) that is implemented in `pytorch` as [`torch.optim.SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) or [ADAM](https://optimization.cbe.cornell.edu/index.php?title=Adam), which is implemented as [`torch.optim.Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam).\n",
        "\n",
        "The inner training loop needs to follow a fixed structure in `pytorch` that looks like this:\n",
        "\n",
        "```python\n",
        "\n",
        "# Send (batch of) inputs through the model\n",
        "model_outputs = model(inputs)\n",
        "\n",
        "# compute the loss of the model outputs using the model predictions and truth labels/target variables\n",
        "loss = loss_function(model_outputs, targets)\n",
        "\n",
        "# make sure gradients are reset to zero in each iteration\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# backpropagate the loss. This will compute the gradient of the loss\n",
        "# w.r.t. the weights of our model, which we need to update the weights\n",
        "loss.backward()\n",
        "\n",
        "# finally, actually update the weights\n",
        "optimizer.step()\n",
        "```\n",
        "\n",
        "This \"inner loop\" is a loop over all the batches in our data, and it ends once we used up all of them.\n",
        "\n",
        "Then there is the \"outer training loop\", which simply repeats the inner loop for a user-specified number of multiple epochs, but it consists of two parts:\n",
        "\n",
        "1. The training part: Here, we just run the model through the inner training loop as described and also keep track of the **average** loss per batch.\n",
        "2. The validation part: We set the model to **evaluation mode** (turn off gradients computation, set specific layers to freeze trained values) and then send our validation set through it. Again we compute the loss (also often in a batched fashion) and in the end compute the average loss per batch.\n",
        "\n",
        "With these two parts, we can plot a **loss curve** of our training: The x axis shows the epoch and the y axis is the loss value. We can draw two curves, one for the training and one for the validation loss. This allows us to detect overfitting (train loss decreases, validation loss stalls out or increases) and also observe the convergence behaviour of our training.\n",
        "\n",
        "This was a lot of theory, let's put this into practice!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In order to be able to use the pytorch dataloader, we need to create pytorch\n",
        "# datasets from our training, validation and test sets (and their respective\n",
        "# target vectors)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_set = TensorDataset(torch.tensor(x_train),\n",
        "                          torch.from_numpy(y_train.to_numpy()).reshape(-1, 1))\n",
        "val_set = TensorDataset(torch.tensor(x_val),\n",
        "                        torch.from_numpy(y_val.to_numpy()).reshape(-1, 1))\n",
        "\n",
        "# create DataLoader objects\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=32)\n",
        "\n",
        "# set number of epochs you want to train.\n",
        "epochs = 10\n",
        "\n",
        "# build model using a single layer with 64 neurons\n",
        "reg_model = Regressor(layers=[64], n_inputs = x_train.shape[1])\n",
        "\n",
        "# Use Adam optimizer to keep track of model parameter updates.\n",
        "# The \"lr\" parameter is the learning rate and describes the \"step size\"\n",
        "# that we take at each weight update in the direction of the largest\n",
        "# negative gradient value. It is again a hyperparameter we need to tune,\n",
        "# but we leave it at the default of 1e-3 for now.\n",
        "optimizer = optim.Adam(reg_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Define loss function. For a regression problem, use mean squared error loss.\n",
        "loss = F.mse_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define empty lists for storage of training and validation losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# outer training loop\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    running_train_loss = 0\n",
        "    \n",
        "    # make sure model is in training mode\n",
        "    reg_model.train()\n",
        "\n",
        "    # training part of outer loop = inner loop\n",
        "    for batch in train_loader:\n",
        "        \n",
        "        data, targets = batch\n",
        "        output = reg_model(data)\n",
        "        tmp_loss = loss(output, targets)\n",
        "        optimizer.zero_grad()\n",
        "        tmp_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_train_loss += tmp_loss.item()\n",
        "    \n",
        "    print(f\"Train loss after epoch {epoch+1}: {running_train_loss/len(train_loader)}\")\n",
        "    train_losses.append(running_train_loss/len(train_loader))\n",
        "    \n",
        "    ## validation part of outer loop\n",
        "    \n",
        "    running_val_loss = 0\n",
        "    # deactivate gradient computation\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        # set model to evaluation mode\n",
        "        reg_model.eval()\n",
        "        \n",
        "        # loop over validation DataLoader\n",
        "        for batch in val_loader:\n",
        "            \n",
        "            data, targets = batch\n",
        "            output = reg_model(data)\n",
        "            tmp_loss = loss(output, targets)\n",
        "            running_val_loss += tmp_loss.item()\n",
        "        \n",
        "        mean_val_loss = running_val_loss/len(val_loader)\n",
        "        print(f\"Validation loss after epoch {epoch+1}: {mean_val_loss}\")\n",
        "        \n",
        "        # If the validation loss of the model is lower than that of all the\n",
        "        # previous epochs, save the model state\n",
        "        if epoch == 0:\n",
        "            torch.save(reg_model.state_dict(), \"./min_val_loss_reg_model.pt\")\n",
        "        elif (epoch > 0) and (mean_val_loss < np.min(val_losses)):\n",
        "            print(\"Lower loss!\")\n",
        "            torch.save(reg_model.state_dict(), \"./min_val_loss_reg_model.pt\")\n",
        "        \n",
        "        val_losses.append(mean_val_loss)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Done training {epochs} epochs!\")\n",
        "print(f\"Training took {end-start:.2f} seconds!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check training convergence\n",
        "\n",
        "Our first training is done! Let's check if it converged nicely: We saved all the training and validation losses in a `list` which we can now plot!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(np.arange(epochs), train_losses, label=\"training\")\n",
        "plt.plot(np.arange(epochs), val_losses, label=\"validation\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems the training converged! However, we see that both training and validation losses still go down, so we could have trained a little longer here probably.\n",
        "\n",
        "We can now do the final step of our analysis and evaluate our model!\n",
        "\n",
        "## Evaluate the trained model\n",
        "\n",
        "The evaluation is done using the **test set**. We send it through the model to get the predictions, and compare them to our truth labels to see how accurate it is. There are several metrics one can use to quantify the model performance. Which methods to use also depends on the task. For a regression task, we can take a look at the mean squared error (i.e. the loss we were minimizing), the mean absolute error or the median absolute error, which is less affected by very large differences. Another score that is often used is the [R2 score](https://en.wikipedia.org/wiki/Coefficient_of_determination), which is defined as\n",
        "\n",
        "$$R^{2}=1-\\frac{\\sum{(y_{i}-\\hat{y}_{i})^{2}}}{\\sum{(y_{i}-\\bar{y})^{2}}}$$\n",
        "\n",
        "where $y$ is the target value, $\\hat{y}$ the value predicted by the model and $\\bar{y}$ the mean of the target values. One can understand this variable as the proportion of the total variance which can be explained by the model.\n",
        "\n",
        "Let's compute some performance measures for our regression model. We can use the scikit-learn package (`sklearn`) again, which already offers functions to conveniently compute them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import (mean_absolute_error,\n",
        "                             mean_squared_error,\n",
        "                             median_absolute_error,\n",
        "                             max_error, r2_score)\n",
        "\n",
        "# make sure the minimum validation loss model is used\n",
        "reg_model.load_state_dict(torch.load(\"./min_val_loss_reg_model.pt\"))\n",
        "\n",
        "y_pred = reg_model.predict(torch.tensor(x_test))\n",
        "\n",
        "print(\"Classification performance report\")\n",
        "\n",
        "print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred):.2f}\")\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "print(f\"Median Absolute Error: {median_absolute_error(y_test, y_pred):.2f}\")\n",
        "print(f\"Max Error: {max_error(y_test, y_pred):.2f}\")\n",
        "print(f\"R2 score: {r2_score(y_test, y_pred):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the results aren't too great. We still have large errors. The largest being about 60, which is very significant if we take into account the average lifespan of a human being. Also, the R2 score is negative, which means our model is not a very good predictor: The difference of the predictions and the target values is so large compared to the difference of the target values with their mean, that the ratio in the above formula becomes larger than one and we are getting a negative value overall.\n",
        "\n",
        "Let's take a look at how well the predicted value correlates with the true value of the life epxectancy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.errorbar(y_test, y_pred, fmt='bo', label=\"True values\")\n",
        "plt.xlabel(\"True Life Expectancy\")\n",
        "plt.ylabel(\"Predicted Life Expectancy\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Well, that doesn't look like a good correlation, either. It will be your task to fix this later!\n",
        "\n",
        "That's it! We now finished a full regression task, using a full \"analysis pipeline\":\n",
        "\n",
        "- loading the data\n",
        "- doing some exploratory data analysis\n",
        "- checking for outliers and cleaning `NaN` and `INF` values\n",
        "- encoding non-numerical values\n",
        "- splitting the data into training, validation and test sets\n",
        "- preprocessing the data for optimised training\n",
        "- running the training and checking convergence\n",
        "- selecting the model with the best generalization performance\n",
        "- evaluating the model\n",
        "\n",
        "\n",
        "Now it's time for you to put into practice what you have learned!\n",
        "\n",
        "## Task 1: Improve the regression model\n",
        "\n",
        "Your first task is to play around with some of the parameters of the analysis pipeline to get better results for the regression. The amount of hyperparameters that can be tuned is vast, so there is surely some room for improvement! Here are some first leads:\n",
        "\n",
        "- In the preprocessing, we used imputation for the missing values. Maybe with a different imputation method to handle `NaN` values, we can improve the performance? One hyperparameter for the kNN imputation is the value of $k$ that is used (default is 5). Maybe this wasn't the optimal value to use here.\n",
        "- We didn't tune any of the model/training hyperparameters here (numbers of nodes/layers, learning rate, activation functions, batch size etc.), this would be a good starting point to improve the baseline performance probably"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2 (Bonus): Predict housing prices in California\n",
        "\n",
        "The second task requires you to put everything you just learned to the test. Using the [California housing prices dataset](https://www.kaggle.com/datasets/camnugent/california-housing-prices) from kaggle (original data from the California 1990 census), your task is to do a regression to predict the median house price (column name is `median_house_value`) in the different districs of California from the different features contained in the dataset (such as the number of bedrooms or bathrooms, proximity to the ocean etc.). \n",
        "\n",
        "Please **do not** download the dataset from kaggle directly, but use the command below in a command shell of your choice to get it (we have slightly changed this dataset to make it a bit more challenging for you):\n",
        "\n",
        "```\n",
        "wget -O housing.csv https://wolke.physnet.uni-hamburg.de/index.php/s/XJ7aCZJ44NTxBQX/download\n",
        "```\n",
        "\n",
        "Don't forget to go through the whole data analysis/cleaning/preprocessing chain before starting to implement your machine learning model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Classification\n",
        "\n",
        "## Finding the Higgs Boson\n",
        "\n",
        "One of the major breakthroughs in Particle Physics in the last decade was the discovery of the Higgs Boson by the CMS and ATLAS collaborations at CERN. One major challenge is to distinguish particle collision events that actually contain the particles we are interested in (such as the Higgs boson) from \"uninteresting\" events that are already well-known in the Standard Model, but might still look similar to interesting events. The interesting events are also referred to as \"signal\" events, while the uninteresting ones are referred to as \"background\" events.\n",
        "\n",
        "For the classification task, we will use the [Higgs dataset](https://archive.ics.uci.edu/ml/datasets/HIGGS#), where we will try to train a classifier using `pytorch` to distinguish signal and background events from various input features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading the Higgs dataset\n",
        "\n",
        "Since the original files of the Higgs dataset are huge, we use a random subset of 25% of the original dataset for this tutorial. For convenience, we prepared two H5 files that contain the low-level kinematic inputs of the particles and higher level features that are computed from the lower level ones, respectively. These files can be downloaded using the following commands from your favourite command shell:\n",
        "\n",
        "```\n",
        "wget -O higgs_highlevels.h5 https://wolke.physnet.uni-hamburg.de/index.php/s/CXBsb63njFZe5J9/download\n",
        "\n",
        "wget -O higgs_lowlevels.h5 https://wolke.physnet.uni-hamburg.de/index.php/s/nr9SREtXkBmzys9/download\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBdPQ8SrkoBN"
      },
      "source": [
        "## The Higgs dataset: Exploration\n",
        "\n",
        "Similar to our regression task, we first do some exploratory data analysis before we start with the analysis. For now, we will focus on the high level variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOGKIo2t4yZH"
      },
      "outputs": [],
      "source": [
        "# load h5 file into pandas dataframe, we will look at the high level variables\n",
        "# at first\n",
        "\n",
        "df_highlvl = pd.read_hdf(\"higgs_highlevels.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at the features contained in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_highlvl.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see there are several mass and energy variables, as well as kinematic variables of leptons. There is also a `class_label` column, which tells us if an event comes from a Higgs decay (\"signal\", label=1) or from a known Standard Model particle decay (\"background\", label=0). In Machine Learning, these truth-level variables are often referred to as *label* or *target* values.\n",
        "\n",
        "Let us take a look at the data types in our data frame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_highlvl.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that all of our variables are already in float format. This is great, because it means that we don't have to deal with encoding non-numerical features here!\n",
        "\n",
        "Next, let's plot the distributions of these variables for the signal and background events separately:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create mask for signal/background distribution comparison\n",
        "sig_mask = (df_highlvl['class_label'] == 1)\n",
        "\n",
        "# drop first column (class_label) and rearrange other columns \n",
        "# to a 6x2 matrix (as numpy array) , which will come in handy\n",
        "# later, when we access the rows and columns of our plot\n",
        "colname_matrix = df_highlvl.columns.to_numpy()[1:].reshape(6,2)\n",
        "fig, axs = plt.subplots(6,2, figsize=(14, 21))\n",
        "\n",
        "for row in range(6):\n",
        "    for col in range(2):\n",
        "        colname_tmp = colname_matrix[row, col]\n",
        "\n",
        "        # set current axis to subplot index\n",
        "        plt.sca(axs[row, col])\n",
        "        \n",
        "        # define bin edges and use histogram plotting function\n",
        "        # of pandas dataframe\n",
        "        bin_edges = np.linspace(df_highlvl[colname_tmp].min(),\n",
        "                                df_highlvl[colname_tmp].max(), 100)\n",
        "\n",
        "        df_highlvl[~sig_mask][colname_tmp].plot.hist(bins=bin_edges,\n",
        "                                                     label=\"background\")\n",
        "\n",
        "        df_highlvl[sig_mask][colname_tmp].plot.hist(histtype=\"step\", bins=bin_edges,\n",
        "                                            label=\"signal\")\n",
        "\n",
        "        plt.xlabel(colname_tmp)\n",
        "\n",
        "        # the phi angular coordinate is typically flat, which is\n",
        "        # why it's the only feature we would like to see not in log scale\n",
        "        if not \"phi\" in colname_tmp:\n",
        "            plt.yscale(\"log\")\n",
        "\n",
        "        plt.legend(loc=\"upper right\")\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLhv4s6ZjeCG"
      },
      "source": [
        "We can see what is often the case in particle physics: Signal and background distributions are - at least by eye - very similar. Neural networks try to find patterns in the data and utilize the subtle changes between signal and background to learn a function that maps from the features we see here to a single variable that has a much higher discriminative power.\n",
        "\n",
        "The strength of Deep Neural Networks (DNNs) is that they can learn nonlinear functions over the full input space and thus can also pick up on correlations in higher dimensions, which we can not see easily by just looking at the one dimensional marginal distributions of the features.\n",
        "\n",
        "## Preprocessing the data\n",
        "\n",
        "We have now looked at the distributions and while we could not see any \"strange\" patterns or outliers in our data, we still need to make sure that there are not too many infinite or NaN values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count NaN or INF values per column\n",
        "(~np.isfinite(df_highlvl)).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, we got lucky this time: There are no INF or NaN values in this dataset. This is often the case in particle physics data analysis, since usually the input data comes in through a sophisticated data preprocessing pipeline that is curated by your research collaboration, but it might be different in other fields.\n",
        "\n",
        "Now we again split our dataset into training, validation and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# At first, we will move the target column to a separate variable and then drop\n",
        "# this column from the original dataframe\n",
        "\n",
        "y_full = df_highlvl['class_label']\n",
        "x_full = df_highlvl.drop(columns=['class_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import train_test_split function from scikit_learn, then first split off\n",
        "# training set, and then split remainder into validation and test sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split off training set (70% of samples)\n",
        "x_train, x_remain, y_train, y_remain = train_test_split(x_full, y_full,\n",
        "                                                        train_size=0.7,\n",
        "                                                        random_state=42)\n",
        "\n",
        "\n",
        "# split remainder into test and validation sets\n",
        "# (15% each, corresponding to half of the remaining non-training samples)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_remain, y_remain,\n",
        "                                                train_size=0.5,\n",
        "                                                random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's see if the numbers check out\n",
        "n_full = x_full.shape[0]\n",
        "train_percent = (x_train.shape[0]/n_full)*100\n",
        "val_percent = (x_val.shape[0]/n_full)*100\n",
        "test_percent = (x_test.shape[0]/n_full)*100\n",
        "\n",
        "print(f\"Train set corresponds to {train_percent:.2f}% of the full data.\")\n",
        "print(f\"Validation set corresponds to {val_percent:.2f}% of the full data.\")\n",
        "print(f\"Test set corresponds to {test_percent:.2f}% of the full data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems everything checks out! Now we again apply some scaling for our input features to have a mean of zero and unit standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import standard scaler from scikit-learn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit (i.e. do the mean/std computation based on the training set features)\n",
        "# and transform training data\n",
        "\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "\n",
        "# important: the standard scaling is always with respect to the training set\n",
        "# values of mean and std! So DO NOT call fit_trainsform again on the other sets\n",
        "# but just transform using the already fitted scaler.\n",
        "\n",
        "x_val = scaler.transform(x_val)\n",
        "x_test = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if it worked\n",
        "print(\"Means of training set:\\n\", x_train.mean(axis=0), \"\\n\")\n",
        "print(\"Standard deviations of training set:\\n\", x_train.std(axis=0), \"\\n\\n\")\n",
        "\n",
        "print(\"Means of validation set:\\n\", x_val.mean(axis=0), \"\\n\")\n",
        "print(\"Standard deviations of validation set:\\n\", x_val.std(axis=0), \"\\n\\n\")\n",
        "\n",
        "print(\"Means of test set:\\n\", x_test.mean(axis=0), \"\\n\")\n",
        "print(\"Standard deviations of test set:\\n\", x_test.std(axis=0), \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems it worked! \n",
        "\n",
        "Now we are good to go and can build the model.\n",
        "\n",
        "## Building a classifier model\n",
        "\n",
        "The good news is: we can keep the exact same architecture as for the regression model and only make some minor changes. The first change is that now we need an activation after the output linear layer. For binary classification, we need the sigmoid activation, which is implemented in [`torch.nn.Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid). This activation function makes sure that the output value is a continuous number between zero and one and can thus be interpreted as a probability how \"signal-like\" the model thinks an event is.\n",
        "\n",
        "Secondly, we need to change the loss function. This time, it is not the mean squared error, but the [binary crossentropy](https://en.wikipedia.org/wiki/Cross_entropy) that is minimized. How can we interpret this loss?\n",
        "\n",
        "A task that occurs very often in science and other fields is [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation), where we estimate the parameters of a probability distribution from data by maximizing the likelihood function. Out of various reasons, one ends up minimizing the negative logarithm of that function (\"negative log-likelihood minimization\"), which is an equivalent problem.\n",
        "\n",
        "A binary classification problem only has two outcomes, in our case \"signal\" and \"background\". This is similar to a coin throw. As we know, this kind of random variable is described by a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) and the binary crossentropy is simply the negative logarithmic likelihood of that:\n",
        "\n",
        "$$\\mathrm{BCE\\_loss}=-\\frac{1}{N}\\sum{y_{i}\\cdot\\log{(\\hat{y}_{i})}+(1-y_{i})\\cdot\\log{(1-\\hat{y}_{i})}}$$\n",
        "\n",
        "where again $\\hat{y}$ is the model prediction, $y$ ist the target value and $i$ is the index of a particular sample/event.\n",
        "\n",
        "So what we are essentially doing is maximizing the likelihood of our data under a Bernoulli distribution. \n",
        "\n",
        "In the code below, we again coded up a class for the `Classifier` model. It is the same as the previous `Regressor` class, except that it uses a sigmoid activation in the output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, layers, n_inputs=5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = []\n",
        "        for nodes in layers:\n",
        "            self.layers.append(nn.Linear(n_inputs, nodes))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            n_inputs = nodes\n",
        "\n",
        "        # the final linear layer with a Sigmoid activation for binary\n",
        "        # classification\n",
        "        self.layers.append(nn.Linear(n_inputs, 1))\n",
        "        self.layers.append(nn.Sigmoid())\n",
        "\n",
        "        self.model_stack = nn.Sequential(*self.layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model_stack(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "            self.eval()\n",
        "            x = torch.tensor(x)\n",
        "            prediction = self.forward(x).detach().cpu().numpy()\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the training\n",
        "\n",
        "Here we can use the exact same training loop as in our regression problem. The only thing we need to change before is the loss function: from mean squared error to binary crossentropy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_set = TensorDataset(torch.tensor(x_train),\n",
        "                          torch.from_numpy(y_train.to_numpy()).reshape(-1, 1))\n",
        "val_set = TensorDataset(torch.tensor(x_val),\n",
        "                        torch.from_numpy(y_val.to_numpy()).reshape(-1, 1))\n",
        "\n",
        "# create DataLoader objects\n",
        "train_loader = DataLoader(train_set, batch_size=1024, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=1024)\n",
        "\n",
        "# set number of epochs you want to train.\n",
        "epochs = 5\n",
        "\n",
        "# build model using a single layer with 64 neurons\n",
        "clsf_model = Classifier(layers=[64], n_inputs = x_train.shape[1])\n",
        "\n",
        "optimizer = optim.Adam(clsf_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Define loss function. For a binary classification problem,\n",
        "# use binary crossentropy loss.\n",
        "loss = F.binary_cross_entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# outer training loop\n",
        "\n",
        "# define empty lists for storage of training and validation losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    running_train_loss = 0\n",
        "    \n",
        "    # make sure model is in training mode\n",
        "    clsf_model.train()\n",
        "\n",
        "    # training part of outer loop = inner loop\n",
        "    for batch in train_loader:\n",
        "        \n",
        "        data, targets = batch\n",
        "        output = clsf_model(data)\n",
        "        tmp_loss = loss(output, targets)\n",
        "        optimizer.zero_grad()\n",
        "        tmp_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_train_loss += tmp_loss.item()\n",
        "    \n",
        "    print(f\"Train loss after epoch {epoch+1}: {running_train_loss/len(train_loader)}\")\n",
        "    train_losses.append(running_train_loss/len(train_loader))\n",
        "    \n",
        "    ## validation part of outer loop\n",
        "    \n",
        "    running_val_loss = 0\n",
        "    # deactivate gradient computation\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        # set model to evaluation mode\n",
        "        clsf_model.eval()\n",
        "        \n",
        "        # loop over validation DataLoader\n",
        "        for batch in val_loader:\n",
        "            \n",
        "            data, targets = batch\n",
        "            output = clsf_model(data)\n",
        "            tmp_loss = loss(output, targets)\n",
        "            running_val_loss += tmp_loss.item()\n",
        "        \n",
        "        mean_val_loss = running_val_loss/len(val_loader)\n",
        "        print(f\"Validation loss after epoch {epoch+1}: {mean_val_loss}\")\n",
        "        \n",
        "        # If the validation loss of the model is lower than that of all the\n",
        "        # previous epochs, save the model state\n",
        "        if epoch == 0:\n",
        "            torch.save(clsf_model.state_dict(), \"./min_val_loss_clsf_model.pt\")\n",
        "        elif (epoch > 0) and (mean_val_loss < np.min(val_losses)):\n",
        "            print(\"Lower loss!\")\n",
        "            torch.save(clsf_model.state_dict(), \"./min_val_loss_clsf_model.pt\")\n",
        "        \n",
        "        val_losses.append(mean_val_loss)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Done training {epochs} epochs!\")\n",
        "print(f\"Training took {end-start:.2f} seconds!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check training convergence\n",
        "\n",
        "Our first classifier training is done! Let's check if it converged nicely:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(np.arange(epochs), train_losses, label=\"training\")\n",
        "plt.plot(np.arange(epochs), val_losses, label=\"validation\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems the training converged! However, we see that both training and validation losses still go down, so we could have trained a little longer here probably.\n",
        "\n",
        "We can now do the final step of our analysis and evaluate our model!\n",
        "\n",
        "## Evaluate the trained model\n",
        "\n",
        "The evaluation is done again using the **test set**. We send it through the model to get the predictions, and compare them to our truth labels to see how accurate it is. \n",
        "\n",
        "The metrics that we use to quantify the model performance are a different from the ones we used for the regression problem. The simplest is the accuracy: the fraction of correctly predicted samples divided by all samples in the test set.\n",
        "\n",
        "Another more sophisticated but very common method is to plot a [`Receiver Operator Characteristic (ROC) curve`](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). The advantage of this approach is that it takes into account both correct predictions of the positive class as well as wrongly classified negative class samples and it does so for different thresholds on the classifier output.\n",
        "\n",
        "A typical approach is to plot this curve and quote its integral, the \"Area Under Curve\" (AUC) value as a single performance metric. The AUC is also often quoted in machine learning research papers to compare the performance of different state-of-the-art model architectures with each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import ROC/AUC computation from scikit-learn package\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
        "\n",
        "# make sure we take the model with the lowest validation loss\n",
        "clsf_model.load_state_dict(torch.load(\"./min_val_loss_clsf_model.pt\"))\n",
        "\n",
        "y_score = clsf_model.predict(x_test)\n",
        "\n",
        "test_acc = accuracy_score(y_test, np.around(y_score))\n",
        "print(f\"Model accuracy: {test_acc:.2f}\")\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
        "\n",
        "test_auc = roc_auc_score(y_test, y_score)\n",
        "print(f\"Model AUC: {test_auc:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot ROC curve\n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot(np.linspace(0,1,100), np.linspace(0,1,100), color=\"grey\",\n",
        "         linestyle=\"dashed\")\n",
        "\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! We did a full analysis run, starting with the raw data, using some preprocessing, then training a binary classifier using `pytorch` and evaluating it.\n",
        "\n",
        "The accuracy is about 70% and the AUC is about 0.8. The dashed line in the ROC curve is simply the diagonal and stands for the worst possible (i.e. random) performance, which corresponds to an AUC of 0.5. Our result is quite higher than that, but still, we might do better. Now it's your turn!\n",
        "\n",
        "## Task 1: Improve the result!\n",
        "\n",
        "Try to improve the AUC and also try to understand how varying different parameters of the training affect the result.\n",
        "\n",
        "Some starting points might be:\n",
        "\n",
        "- So far, we only considered a single hidden layer with 64 nodes. Maybe you can improve the model architecture a bit?\n",
        "- We didn't play with other hyperparameters, like:\n",
        "    - the learning rate\n",
        "    - batch size\n",
        "    - the optimizer\n",
        "    - activations for the hidden layers\n",
        "- We also didn't use the low-level features that are in the `higgs_lowlevels.h5`. Using the same preprocessing procedure as before, maybe you can integrate (some of) those and see if they actually improve our performance.\n",
        "\n",
        "Play around a bit and get to know the `pytorch` library better! Feel free to copy code from anything we used so far!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Breast cancer detection\n",
        "\n",
        "Breast cancer is a serious health issue, since it is the most common form of cancer amongst women in the world and accounts for about 25% of all cancer cases. A key challenge is to classify tumors into malignant (i.e. cancerous) tumors and benign (i.e. non cancerous) tumors.\n",
        "\n",
        "Your task ist to use the [breast cancer dataset](https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset) and train a classifier to distinguish between these two kinds of tumors given various tumor properties. You can either download the data from [kaggle](https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset) or use the below command in a command shell.\n",
        "\n",
        "```\n",
        "wget -O breast_cancer.csv https://wolke.physnet.uni-hamburg.de/index.php/s/2W74iN84D9kCkTc/download\n",
        "```\n",
        "\n",
        "Run the whole analysis pipeline (exploratory data analysis, data cleaning and preprocessing, training and evaluation) for this dataset and get a first baseline AUC for your model, then subsequently try to improve it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bonus: Computer vision task\n",
        "\n",
        "We started our journey with two simple problems: binary classification and regression. However, we can use the power of deep learning for far more advanced tasks. In particular, we can use it to detect patterns on image-based data!\n",
        "\n",
        "We start with a rather simple dataset: the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). This is a dataset of images of hand-written digits and the task is to classify each image to the respective digit it is representing. This task will be different in two ways from the previous tasks: firstly, we will use a different network architecture, a `convolutional neural network` and secondly, we now have a **multi-class** classification task, since now we need to classify into 10 classes, i.e. we need to say if an image shows a digit from 0 to 9.\n",
        "\n",
        "This task is also a good example where we can start using the [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) module to load our data. This is very useful, since we do not need to care too much about data cleaning and preprocessing ourselves. In fact, we even get data that is already split into training and validation sets, though not into a separate test set, unfortunately. For this tutorial, we will make our lives a bit easier and just use training and validation set, \"cheat\" a bit and do the final evaluation on the validation set too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import the mnist dataset and a transformation library\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a new MNIST object. This is essentially a pytorch dataset object\n",
        "# containing the images of the hand written digits.\n",
        "# The first argument of this object is the root directory where the data is \n",
        "# stored. If you did not download it already, you can simply tell torchvision\n",
        "# to download it for you, by setting the respective parameter to True.\n",
        "# We also set train to True, which means in this case we get the \n",
        "# dataset for the training set.\n",
        "mnist_dataset_train = MNIST(\"./mnist\", train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "mnist_dataset_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the dataset object prints out some specific information: we have 60,000 datapoints, the root location is `\"./mnist\"` and we use the training split of the full dataset. To access the actual data inside the dataset, we need to use the `data` variable. Let's take a look at its shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist_dataset_train.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, we have 60,000 images and each image has 28 x 28 pixels. Lets pick out a single image and use `plt.imshow` to plot the respective digit in greyscale:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.imshow(mnist_dataset_train.data[1], cmap=\"gray_r\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I guess we can safely assume we found a zero here! This is very good. Now let's also create an `MNIST` object for the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist_dataset_val = MNIST(\"./mnist\", train=False, transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can start to build our first convolutional neural network model. The topic of machine learning for computer vision tasks is so large that you can probably have semester-filling lectures about it. So for this quick tutorial we will try to only cover the very foundations of convolutional neural networks.\n",
        "\n",
        "## Convolutions: Introduction\n",
        "\n",
        "Convolutional neural networks are specifically tailored to perform well on a specific type of data, which is **image data**. There are two key insights that motivated the introduction of convolutions for image data: firstly, neighboring pixels in an image are usually highly correlated. Let's say you have an image of a cat lying on a grass hill with some blue sky in the background. If you took any of the blue pixels from the sky at random, the probability that another blue pixel from the sky is next to it is extremely high, while it is very unlikely that any of the neighboring pixels contain information about either the grass or the cat. From this fact, it is only logical that methods operating on image data do not need to capture information from the *whole* image, but rather from coherent *patches* inside the image. In our example image, there are three patches: some area will have a lot of grass, some other area will have a lot of blue sky and the third one contains the cat.\n",
        "\n",
        "Secondly: Image data is *translation invariant*. Let's go again with the previous example image. Say we would like to predict from some image dataset if an image contains a cat or not. The information *where* the cat is in this image is irrelevant for this task. It can be in the upper left corner of the image or in the center or in any other position. If we would shift all image pixels up or down or left or right, we would end up with an image where the cat is now at a different position, but a machine learning algorithm should still be able to classify the image as containing a cat. This feature of image data leads to the assumption that a machine learning algorithm would probably benefit from a method that is somehow scanned over the image to be able to recognize patterns at any position of the image.\n",
        "\n",
        "To summarize:\n",
        "- images are typically made up of highly correlated, local patches of pixels -> motivates method that looks at smaller region of image rather than trying to capture the information of the image as a whole\n",
        "- images are translation invariant, the location of an object inside an image should not matter -> motivates method that is scanned over image to recognize patterns everywhere\n",
        "\n",
        "## Convolutions in practice\n",
        "\n",
        "Convolutions combine both points: They use small so-called \"kernels\" that are systematically scanned over the image and each \"pixel value\" of the kernel is a parameter of the model that is adjusted during the training. Some of you might have heard of convolutions in mathematics, i.e. the convolution of two functions ( $f*g$ ). This is a completely different topic, however, and what we do in image processing is quite different. Here is a quick example how a convolutional layer in a machine learning model operates:\n",
        "\n",
        "Consider the 3x3 kernel being convoluted with the 5x5 pixel image on the right.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://s3.desy.de/hackmd/uploads/upload_5aa3677a95d34d07fab6e31f62cb5bfe.png\" width=60%>\n",
        "</p>\n",
        "\n",
        "The convolution operation is done like this: we overlay the kernel with the image, starting in the top left corner. We multiply each element in the kernel with each corresponding element in the image and then sum everything up and set the result as the value of a new output image pixel, again starting from the top left. See the illustration below:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://s3.desy.de/hackmd/uploads/upload_a2909f900730f9d2312d0573b0454a90.png\" width=80%>\n",
        "</p>\n",
        "\n",
        "We then repeat this process for each possible overlaid position of the kernel inside the image, going forward in a left-to right, top-to-bottom manner. Find below the illustration of the second step:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://s3.desy.de/hackmd/uploads/upload_92689504280872629ae0450e09358996.png\" width=80%>\n",
        "</p>\n",
        "\n",
        "Finally, after we iterated all possible positions, the output will look like this:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://s3.desy.de/hackmd/uploads/upload_20cd4749951bdb8153974a160a5f26ca.png\" width=30%>\n",
        "</p>\n",
        "\n",
        "Convolutions are useful because they are able to detect specific structures in our image: The kernel we just used, being one in the central column but zero everywhere else, produces a high output value if three pixels that are in the same column all have high values (high values in an image meaning a high intensity). This means we can use this kernel as a \"feature detector\" which detects straight vertical lines. This might be helpful in the MNIST case for detecting hand-written ones.\n",
        "\n",
        "This is a very simple example, but once you start building a deep convolutional network with many convolutional layers and many kernels per layer, we can train our kernels to detect very specific features, such as a nose or an eye in a face recognition task etc. So to summarize: kernels can be trained to detect specific features using convolutions, and the output of each convolution tells us if such a feature is present in the convoluted image, by showing a high value in any of the output pixels.\n",
        "\n",
        "## (Max) Pooling layers\n",
        "\n",
        "Another common method used in convolutional neural networks is pooling. They are often applied after a convolution operation. Pooling methods simply aggregate information in an image. Let's say you tried to classify landscape images and many images contain large patches of blue sky. This means there is a lot of redundant information in the image: of course, you are interested if there is blue sky in the image, but you don't need a huge amount of pixels conveying this information.\n",
        "\n",
        "This is where pooling comes in. In pooling, you also scan a kernel over the image and then map all the values that are overlaid with the kernel to a single, aggregated value. For example, if you used max pooling layers, you would just take the maximum value in that area. Other than convolutions, in pooling you often scan through the image in a non-verlapping way, so you set the \"stride\" to the respective kernel size such that the kernels are placed exactly next to each other. Here is an image that shows how max pooling works in practice:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://s3.desy.de/hackmd/uploads/upload_575d3be1d9f86cd766423ea1fd0a79e7.png\" width=60%>\n",
        "</p>\n",
        "\n",
        "\n",
        "Now that we learned the foundations of convolutions, let's put the theory into practice and build our first convolutional neural network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Convoluter(nn.Module):\n",
        "    def __init__(self, fcn_layers, conv_layers, kernel_size,\n",
        "                 input_dim=28, in_channels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # similar to the classifier and regressor models,\n",
        "        # each number in layers will set the parameters\n",
        "        # of one model layer.\n",
        "        self.layers = []\n",
        "        \n",
        "        for kernels in conv_layers:\n",
        "            # structure of Conv2d:\n",
        "            # first argument: number of input channels\n",
        "            # (corresponds to number of kernels from \n",
        "            # previous layer)\n",
        "            # second argument: number of kernels in current layer\n",
        "            # third argument: kernel size (tuple or single integer for\n",
        "            # quadratic kernels)\n",
        "            self.layers.append(nn.Conv2d(in_channels, kernels, kernel_size))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            \n",
        "            # add max pooling\n",
        "            self.layers.append(nn.MaxPool2d(2,2))\n",
        "            in_channels = kernels\n",
        "            \n",
        "            # keep track of image size such that we can easily compute the\n",
        "            # number of nodes of the first fully connected layer after the\n",
        "            # convolutions later\n",
        "            h = np.floor((input_dim - kernel_size + 1)/2).astype(int)\n",
        "            input_dim = h\n",
        "        \n",
        "        # flatten convoluted outputs and continue with fully connected network\n",
        "        self.layers.append(nn.Flatten())\n",
        "        n_inputs = h*h*in_channels\n",
        "        for nodes in fcn_layers:\n",
        "            self.layers.append(nn.Linear(n_inputs, nodes))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            n_inputs = nodes\n",
        "\n",
        "        # multi-classification into 10 integers needs\n",
        "        # 10 output nodes\n",
        "        self.layers.append(nn.Linear(n_inputs, 10))\n",
        "        \n",
        "        # Softmax layer to make sure all outputs are numbers\n",
        "        # between 0 and 1 and their values sum to 1, such that\n",
        "        # we can interpret them as class probabilities\n",
        "        self.layers.append(nn.Softmax())\n",
        "\n",
        "        # build pytorch model as sequence of our layers\n",
        "        self.model_stack = nn.Sequential(*self.layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # the forward call just takes data (x) and sends it through the model\n",
        "        # to produce an output (in our case a number between 0 and 1)\n",
        "        return self.model_stack(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        # the predict method sets the model to evaluation mode and only then\n",
        "        # computes the model prediction of given data (x). For convenience,\n",
        "        # we already make sure that the output prediction is a numpy array, so\n",
        "        # we can use it easier in the final evaluation of the model.\n",
        "        with torch.no_grad():\n",
        "            self.eval()\n",
        "            x = torch.tensor(x)\n",
        "            prediction = self.forward(x).detach().cpu().numpy()\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare model and optimizer for training\n",
        "\n",
        "The training is done as usual in `pytorch`: we need to create a model and an optimizer, and then we run a training loop. For a multi-class classification task such as MNIST, we use the (categorical) cross entropy loss, which is also defined in `torch.nn.functional`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define dataloaders for MNIST datasets\n",
        "train_loader = DataLoader(mnist_dataset_train, batch_size=16)\n",
        "val_loader = DataLoader(mnist_dataset_val, batch_size=16)\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "# build model using a single layer with 64 neurons\n",
        "conv_model = Convoluter(fcn_layers=[16], conv_layers=[3], kernel_size=3)\n",
        "\n",
        "optimizer = optim.Adam(conv_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Define loss function. For a binary classification problem,\n",
        "# use binary crossentropy loss.\n",
        "loss = F.cross_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# outer training loop\n",
        "\n",
        "# define empty lists for storage of training and validation losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    running_train_loss = 0\n",
        "    \n",
        "    # make sure model is in training mode\n",
        "    conv_model.train()\n",
        "\n",
        "    # training part of outer loop = inner loop\n",
        "    for batch in train_loader:\n",
        "        \n",
        "        data, targets = batch\n",
        "        output = conv_model(data)\n",
        "        tmp_loss = loss(output, targets)\n",
        "        optimizer.zero_grad()\n",
        "        tmp_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_train_loss += tmp_loss.item()\n",
        "    \n",
        "    print(f\"Train loss after epoch {epoch+1}: {running_train_loss/len(train_loader)}\")\n",
        "    train_losses.append(running_train_loss/len(train_loader))\n",
        "    \n",
        "    ## validation part of outer loop\n",
        "    \n",
        "    running_val_loss = 0\n",
        "    # deactivate gradient computation\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        # set model to evaluation mode\n",
        "        conv_model.eval()\n",
        "        \n",
        "        # loop over validation DataLoader\n",
        "        for batch in val_loader:\n",
        "            \n",
        "            data, targets = batch\n",
        "            output = conv_model(data)\n",
        "            tmp_loss = loss(output, targets)\n",
        "            running_val_loss += tmp_loss.item()\n",
        "        \n",
        "        mean_val_loss = running_val_loss/len(val_loader)\n",
        "        print(f\"Validation loss after epoch {epoch+1}: {mean_val_loss}\")\n",
        "        \n",
        "        # If the validation loss of the model is lower than that of all the\n",
        "        # previous epochs, save the model state\n",
        "        if epoch == 0:\n",
        "            torch.save(conv_model.state_dict(), \"./min_val_loss_conv_model.pt\")\n",
        "        elif (epoch > 0) and (mean_val_loss < np.min(val_losses)):\n",
        "            print(\"Lower loss!\")\n",
        "            torch.save(conv_model.state_dict(), \"./min_val_loss_conv_model.pt\")\n",
        "        \n",
        "        val_losses.append(mean_val_loss)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Done training {epochs} epochs!\")\n",
        "print(f\"Training took {end-start:.2f} seconds!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We did it! We trained our first convolutional neural network!\n",
        "\n",
        "## Evaluate the training\n",
        "\n",
        "Let's start bei looking at the losses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(np.arange(epochs), train_losses, label=\"training\")\n",
        "plt.plot(np.arange(epochs), val_losses, label=\"validation\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This looks good, but we could have probably trained a little longer here.\n",
        "\n",
        "Now about the performance: For multi-class classifications, the performance evaluation isn't actually that easy. The simplest thing we can do is estimate the accuracy, i.e. the ratio of correctly predicted images and all images.\n",
        "\n",
        "What one can also plot is a confusion matrix. The rows of this matrix contain the *actual* truth-labelled classes and the columns the classes predicted by the model. In an ideal classifier, we would get a perfect diagonal: all images that were actually a hand-written 0 are assigned to 0, all images that were a 1 are assigned to 1 and so on. However, the classifier will never be perfect, so we will have always values in the off-diagonal elements of that matrix. Let's take a look!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make sure we take the model with the lowest validation loss\n",
        "conv_model.load_state_dict(torch.load(\"./min_val_loss_conv_model.pt\"))\n",
        "\n",
        "# use the full MNIST validation set at once for evaluation\n",
        "# we need to add a single dimension of 1 as the \"channels\" and cast it to\n",
        "# float for the network model to be able to digest this (in our actual\n",
        "# training, this is taken care of by the ToTensor transformation already)\n",
        "full_mnist_val = mnist_dataset_val.data.reshape((-1, 1, 28, 28)).type(torch.float32)\n",
        "\n",
        "preds = conv_model.predict(full_mnist_val)\n",
        "\n",
        "predicted_class = np.argmax(preds, axis=1)\n",
        "true_class = mnist_dataset_val.targets.numpy()\n",
        "\n",
        "accuracy = np.count_nonzero(predicted_class == true_class)/true_class.shape[0]\n",
        "\n",
        "print(f\"The achieved accuracy is: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Very good! In our first test, we already achieved around 90% accuracy! Let's take a look at the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# get confusion matrix\n",
        "conf_matrix = confusion_matrix(true_class, predicted_class)\n",
        "\n",
        "sns.heatmap(conf_matrix, cmap=\"Blues\", annot=True, fmt='g')\n",
        "plt.show()\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This looks good! We see that the values are highest at the diagonals of the matrix. The true class is the rows and the predicted class the columns. We also see some interesting off-diagonal elements.\n",
        "\n",
        "Now it's your turn again! \n",
        "\n",
        "## Task 1: Improve the result\n",
        "\n",
        "Similar to the previous tasks, play around with the network and training hyperparameters to improve the accuracy of the model. Once you have the best version of your model, plot the confusion matrix again. Do the misclassifications change compared to the first version?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Compare performance on fully connected network\n",
        "\n",
        "You've learnt already about fully connected classifiers in this tutorial. You might ask the quesion: Can't I just flatten the image completely and feed the pixel values into a fully connected network?\n",
        "\n",
        "The answer is: of course! Please do this test and compare the performance of the fully connected network with the convolutional one? Which one is better and what do you think is the reason why? Note that you need to change the output layer and activation as well as the loss function of the original `Classifier` model for this to work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: CIFAR 10 dataset\n",
        "\n",
        "Your next task is to run a training on a different dataset: the [CIFAR 10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), which is a dataset of 60000 32x32 pixel colour images that need to be assigned to any of 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck). Use the full analysis pipeline for convolutional neural networks as we discussed in the tutorial. Train and evaluate your model, compute the accuracy, the confusion matrix and then improve until you get the best possible performance!\n",
        "\n",
        "Note that this dataset is also available as a `torchvision` dataset: [`torchvision.datasets.CIFAR10`](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "pytorch_tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('tutorial_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "7230a7bb61bfe9008a0e0cc8b26903332be3b91464d63aba4f20fa83f7afb336"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
